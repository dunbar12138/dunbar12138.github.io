
<!-- saved from url=(0043)http://www.cs.cmu.edu/~aayushb/Recycle-GAN/ -->
<html class="gr__cs_cmu_edu"><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252"><script src="./Recycle-GAN_files/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script><script src="./Recycle-GAN_files/jquery.min.js" type="text/javascript"></script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		        15px 15px 0 0px #fff, /* The fourth layer */
		        15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		        20px 20px 0 0px #fff, /* The fifth layer */
		        20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		        25px 25px 0 0px #fff, /* The fifth layer */
		        25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
	    top: 50%;
	    transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>


  
		<title>Audiovisual Synthesis</title>
		<meta property="og:image/jpg" content="https://www.ri.cmu.edu/wp-content/uploads/2017/01/RI_no-text_small.jpg">
		<meta property="og:title" content="Audiovisual Synthesis ">
  </head>

  <body data-gr-c-s-loaded="true">
    <br>
          <center>
          	<span style="font-size:42px">Unsupervised Any-to-Many Audiovisual Synthesis via Exemplar Autoencoders</span>
	  		  <table align="center" width="600px">
	  			  <tbody><tr>
                <td align="center" width="100px">
                    <center>
                      <span style="font-size:20px"><a href="https://dunbar12138.github.io/">Kangle Deng</a></span>
                      </center>
                </td>
	  	              <td align="center" width="100px">
	  					<center>
	  						<span style="font-size:20px"><a href="http://www.cs.cmu.edu/~aayushb/">Aayush Bansal</a></span>
		  		  		</center>
		  		  	  </td>
	  	          <td align="center" width="100px">
	  					<center>
	  						<span style="font-size:20px"><a href="http://www.cs.cmu.edu/~deva/">Deva Ramanan</a></span>
		  		  		</center>
		  		  	  </td>
			  </tr></tbody></table>

	  		  <table align="center" width="500px">
	  			  <tbody><tr>
	  	              <td align="center" width="100px">
	  					<center>
	  						<span style="font-size:24px"><a href="http://www.cs.cmu.edu/~aayushb/Recycle-GAN/recycle_gan.pdf"> [Paper]</a></span>
		  		  		</center>
		  		  	  </td>

                              <td align="center" width="100px">
                                                <center>
                                                        <span style="font-size:24px"><a href="https://github.com/aayushbansal/Recycle-GAN"> [Code]</a></span>
                                                </center>
                                          </td>
	  			  </tr>
			  </tbody></table>


          </center>

  		  <br>
  		  <table align="center" width="850px">
  			  <tbody><tr>
  	              <td width="400px">
  					<center>
  	                	<a href="./Recycle-GAN_files/teaser.png"><img class="rounded" src="./Recycle-GAN_files/teaser.png" height="350px"></a><br>
					</center>
  	              </td>
                </tr>
  	              <tr><td width="400px">
  					<center>
  	                	<span style="font-size:14px"><i>Our approach for video retargeting used for faces and flowers. The top row shows translation from John Oliver to Stephen Colbert. The bottom row shows how a synthesized flower follows the blooming process with the input flower.</i>
					</span></center>
  	              </td>

  		  </tr></tbody></table>

      	  <br>
		  <hr>


  		  <center><h1>Abstract</h1></center><table align="center" width="850px">
	  		  
	  		  <tbody><tr>
	  		     <td>
                We present an unsupervised approach that enables us to convert the speech input of any one individual to an output set of potentially-infinitely many speakers, i.e., one can stand in front of a mic and be able to make their favorite celebrity say the same words. Our approach builds on simple autoencoders that project out-of-sample data to the distribution of the training set (motivated by PCA/linear autoencoders). We use an exemplar autoencoder to learn the voice and specific style (emotions and ambiance) of a target speaker. In contrast to existing methods, the proposed approach can be easily extended to an arbitrarily large number of speakers in a very little time using only two-three minutes of audio data from a speaker. We also exhibit the usefulness of our approach for generating video from audio signals and vice-versa. <br> <br>
	
	  		    </td>
	  		  </tr>
			</tbody></table>

  		  <br><br>

			<table align="center" width="600px">
  			  <tbody><tr>
				  <td><a href="https://arxiv.org/abs/1808.05174"><img class="layered-paper-big" style="height:175px" src="./images/AudioRetargeting_firstpage.jpg"></a></td>
				  <td><span style="font-size:14pt">K. Deng, A. Bansal, D. Ramanan<br>
          Unsupervised Any-to-Many Audiovisual Synthesis via Exemplar Autoencoders.<br>
				  On ArXiv, 2019.<br>
				  </span>
				  </td>
  	              
              </tr>
  		  </tbody></table>
		 <br>
			 <table align="center" width="600px">
			  <tbody><tr>
				  <td><span style="font-size:14pt"><center>
				  	<a href="http://www.cs.cmu.edu/~aayushb/Recycle-GAN/bibtex.txt">[Bibtex]</a>
  	              </center></span></td>
              </tr>
  		  </tbody></table>



		  <hr>

		                  <center><h1>Resurrecting the past</h1></center><table align="center" width="850px">
                          
                          <tbody><tr>
                             <td>
				We extended Recycle-GAN to use it to generate hi-res videos using 2-3 seconds long low-res video clips of celebrities from past. Here is an example of Winston Churchill narrating the famous speech delivered on June 04, 1940 in British Parliament. 

                            </td>
                          </tr>
                        </tbody></table>

                        <table align="center" width="850px">
                          <tbody><tr>
                      <td width="400px">
                                        <center>

<p><iframe width="560" height="315" src="./Recycle-GAN_files/desao-au0UE.html" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></p>
                                                                <span style="font-size:14px"><i>Thanks to friends at BBC Studios London for providing the data and all the help in creating this video. </i> <br><br><br>
                                        </span></center>
                      </td>
                </tr>
<tr><td width="400px">

			<center> Additional example of Marilyn Monroe:</center><br>

                        <table align="center" width="850px">
                          <tbody><tr>
                      <td width="400px">
                                        <center>

<p><iframe width="560" height="315" src="./Recycle-GAN_files/vjWRk4J9_sY.html" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></p>
                                                                <span style="font-size:14px"><i>Thanks to friends at NBC News for help in creating this video. </i> <br><br>
                                        </span></center>
                      </td>
                </tr>
<tr><td width="400px">
<hr>



                  <center><h1>Face-to-Face</h1></center><table align="center" width="850px">
                          
                          <tbody><tr>
                             <td>
				We use the publicly available videos of various public figures for the face-to-face translation task. The faces are extracted using the facial keypoints generated using the <a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose">OpenPose</a>. We show example videos of face-to-face translation for public figures such as Martin Luther King Jr. (MLK), Barack Obama, John Oliver, Stephen Colbert.<br> <br>
				
                            </td>
                          </tr>
                        </tbody></table>

                        <table align="center" width="850px">
                          <tbody><tr>
                      <td width="400px">
                                        <center>                                       

<!-- <p><iframe width="560" height="315" src="https://www.youtube.com/embed/vjWRk4J9_sY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></p>
                                                                <span style="font-size:14px"><i>Generated using 3-4 seconds of original Marilyn Monroe. </i> <br> -->

<p><iframe width="560" height="315" src="./Recycle-GAN_files/NuXILOS8dKA.html" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></p>
                                                                <span style="font-size:14px"><i>One minute video by WQED.  </i> <br>


 <p><iframe width="560" height="315" src="./Recycle-GAN_files/IpwSklQlUhw.html" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></p>
								<span style="font-size:14px"><i>Generated video (512x512 resolution) of US President Donald Trump.  </i> <br>
											<p><iframe width="560" height="315" src="./Recycle-GAN_files/RTsOUne7RN8.html" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></p>
                                                                <span style="font-size:14px"><i>Edited background of the above video used in the Not-the-white-house-correspondent-dinner.  </i> <br>

											 <p><iframe width="560" height="315" src="./Recycle-GAN_files/QdWWUOm3yxY.html" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></p>

                                        </span></span></span></center>
                      </td>
                </tr>
<tr><td width="400px">
                                        <center>
                                <br>                                <span style="font-size:14px"><i>Another video (512x512 resolution) of US President Donald Trump.  </i> <br>

					<p><iframe width="560" height="315" src="./Recycle-GAN_files/mxu4Ki774TU.html" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></p>

					Above is the video from the show.

                                        </span></center>
                      </td>

                  </tr></tbody></table>

                  <br><br>

                        <table align="center" width="850px">
                          <tbody><tr>
                      <td width="400px">
                                        <center>                                        <p><iframe width="560" height="315" src="./Recycle-GAN_files/1hCXMFVK8PA.html" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></p>

                                        </center>
                      </td>
                </tr>
<tr><td width="400px">
                                        <center>
                                <br>                                <span style="font-size:14px"><i>The above video (512x512) shows facial retargeting from a French Journalist (francetv.fr) to the French President Emmanuel Macron.  The best part of the video are the last 5 seconds. Enjoy the video in HD, full screen mode on your laptop! </i> <br>Thanks to Louis Milanodupont from francetv.fr for sharing the data.
                                        </span></center>
                      </td>

                  </tr></tbody></table>

                  <br><br>

                        <table align="center" width="850px">
                          <tbody><tr>
                      <td width="400px">
                                        <center>                                        <p><iframe width="560" height="315" src="./Recycle-GAN_files/VWXFqDdqLbE.html" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></p>

                                        </center>
                      </td>
                </tr>
<tr><td width="400px">
                                        <center>
                                <br>                                <span style="font-size:14px"><i>The above video (256x256) shows translation from John Oliver to Stephen Colbert. </i>
                                        </span></center>
                      </td>

                  </tr></tbody></table>

                  <br><br>

                        <table align="center" width="850px">
                          <tbody><tr>
                      <td width="400px">
                                        <center>                                        <p><iframe width="560" height="315" src="./Recycle-GAN_files/EU4BvhtEuG0.html" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></p>

                                        </center>
                      </td>
                </tr>
<tr><td width="400px">
                                        <center>                                <br>                                <span style="font-size:14px"><i>
Recreated a full 15 minute show of Last Week Tonight featuring John Oliver and Stephen Colbert using the publicly available Recycle-GAN code. No cherry picking! A 320x320 resolution was generated using a relatively shallower model, hence some jumps here and there.  </i>
                                        </span></center>
                      </td>

                  </tr></tbody></table>

                  <br><br>

                        <table align="center" width="850px">
                          <tbody><tr>
                      <td width="400px">
                                        <center>                                        <p><iframe width="560" height="315" src="./Recycle-GAN_files/8OqxXt_Y2Ik.html" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></p>

                                        </center>
                      </td>
                </tr>
<tr><td width="400px">
                                        <center>
                                <br>                                <span style="font-size:14px"><i>The above video shows a face to face translation from Martin Luther King Jr. (MLK) to Barack Obama.</i>
                                        </span></center>
                      </td>

                  </tr></tbody></table>

                  <br><br>

                        <table align="center" width="850px">
                          <tbody><tr>
                      <td width="400px">
                                        <center>                                        <p><iframe width="560" height="315" src="./Recycle-GAN_files/JHS1n5X4lVw.html" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></p>

                                        </center>
                      </td>
                </tr>
<tr><td width="400px">
                                        <center>
                                <br>                                <span style="font-size:14px"><i>The above video shows a face to face translation from Martin Luther King Jr. (MLK) to Barack Obama. This is similar to previous video but with audio.</i>
                                        </span></center>
                      </td>

                  </tr></tbody></table>

                  <br><br>

                        <table align="center" width="850px">
                          <tbody><tr>
                      <td width="400px">
                                        <center>                                        <p><iframe width="560" height="315" src="./Recycle-GAN_files/1OkSxqaj8vo.html" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></p>

                                        </center>
                      </td>
                </tr>
<tr><td width="400px">
                                        <center>
                                <br>                                <span style="font-size:14px"><i>The above video (320x320) shows translation from John Oliver to Stephen Colbert. </i>
                                        </span></center>
                      </td>

                  </tr></tbody></table>

                  <br><br>

                        <table align="center" width="850px">
                          <tbody><tr>
                      <td width="400px">
                                        <center>                                        <p><iframe width="560" height="315" src="./Recycle-GAN_files/y7nFhmIR47o.html" frameborder="0" allowfullscreen=""></iframe></p>

                                        </center>
                      </td>
                </tr>
<tr><td width="400px">
                                        <center>
				<br>
                                <span style="font-size:14px"><i>The above video shows random examples of face-to-face translation using our approach for various public figures and characters. Without any input alignment or manual supervision, our approach could capture stylistic expressions for these public figures. As an example, John Oliver's dimple while smiling, the shape of mouth characteristic for Donald Trump, and the facial mouth lines and smile of Stephen Colbert etc.</i>
                                        </span></center>
                      </td>

                  </tr></tbody></table>

<br><br><center>We show a few comparison of our approach with Cycle-GAN for face retargeting.</center><br><br>

                        <table align="center" width="850px">                          <tbody><tr>
                      <td width="400px">
                                        <center>
                                        <p><iframe width="560" height="315" src="./Recycle-GAN_files/F51RCdDIuUw.html" frameborder="0" allowfullscreen=""></iframe></p>

                                        </center>
                      </td>
                </tr>

                  </tbody></table>
		<br> <br>

                        <table align="center" width="850px">                          <tbody><tr>
                      <td width="400px">
                                        <center>                                        <p><iframe width="560" height="315" src="./Recycle-GAN_files/UXjWWy6iTVo.html" frameborder="0" allowfullscreen=""></iframe></p>

                                        </center>
                      </td>
                </tr>

                  </tbody></table>

                <br> <br>

                        <table align="center" width="850px">                          <tbody><tr>
                      <td width="400px">
                                        <center>                                        <p><iframe width="560" height="315" src="./Recycle-GAN_files/IkmhU2UmgqM.html" frameborder="0" allowfullscreen=""></iframe></p>

                                        </center>
                      </td>
                </tr>

                  </tbody></table>


                  <br><br>
                  <hr>

                  <center><h1>Body-to-Body</h1></center><table align="center" width="850px">
                          
                          <tbody><tr>
                             <td>

			The above examples are specifically focussed on faces. Here we use the same approach for the body retargeting.<br><br>


                        <table align="center" width="850px">
                          <tbody><tr>
                      <td width="400px">
                                        <center>
                                        <p><iframe width="560" height="315" src="./Recycle-GAN_files/HcCJM36YCnU.html" frameborder="0" allowfullscreen=""></iframe></p>

                                        </center>
                      </td>
                </tr>

                  </tbody></table>
  <center>
                                <br>                                <span style="font-size:14px"><i>The above video (256x256) shows translation from MLK to Barack Obama. The fine details on faces (such as mouth movement) are missing due to low-res input-output. Generating hi-res outputs can potentially enable us to generate such fine details. </i>
                                        </span></center>




                            </td>
                          </tr>
                        </tbody></table>

                  <br>
                  <hr>


                  <center><h1>Flower-to-Flower</h1></center><table align="center" width="850px">
                          
                          <tbody><tr>
                             <td>

Extending from faces and other traditional translations, we demonstrate our approach for flowers. We use various flowers, and extracted their time-lapse from publicly available videos. The time-lapses show the blooming of different flowers but without any sync. We use our approach to align the content, i.e. both flowers bloom or die together. We show a few example videos for different flowers here.<br><br>

	
			<table align="center" width="850px">
                          <tbody><tr>
                      <td width="400px">
                                        <center>
					<p><iframe width="560" height="315" src="./Recycle-GAN_files/m2qEApmAgEI.html" frameborder="0" allowfullscreen=""></iframe></p>

                                        </center>
                      </td>
                </tr>

                  </tbody></table>

<br><br>We show a comparison of our approach with Cycle-GAN using the Dandelion flower. Our approach could learn appropriate correspondences in two domains.<br><br>

                        <table align="center" width="850px">                          <tbody><tr>
                      <td width="400px"> 
                                        <center>
                                        <p><iframe width="560" height="315" src="./Recycle-GAN_files/t-G7Latb1ZM.html" frameborder="0" allowfullscreen=""></iframe></p>

                                        </center>
                      </td>
                </tr>

                  </tbody></table>


                            </td>
                          </tr>
                        </tbody></table>

                  <br>
                  <hr>

                  <center><h1>Video Manipulation via Retargeting</h1></center><table align="center" width="850px">
			
			<tbody><tr>
                             <td>
                                We show our approach for automatic video manipulation via video retargeting for two cases: (1). Synthesizing clouds and winds in videos; (2). making sunrise and sunset in different videos.<br><br>
			
                     	     </td>
                	</tr>
		</tbody></table>
	
		 <center><h2>Clouds &amp; Wind Synthesis</h2></center><table align="center" width="850px">

                          
                          <tbody><tr>
                             <td>
				Our approach can be used to synthesize a new video that has the required environmental condition such as clouds and wind  without the need for physical efforts of recapturing. We use the given video and video data from required environmental condition as two domains in our experiment. The conditional video and trained translation model is then used to generate a required output. <br><br>

For this experiment,  we collected the video data for various wind and cloud conditions, such as calm day or windy day. Using our approach, we can convert a calm-day to a windy-day, and a windy-day to a calm-day, without modifying the aesthetics of the place. <br><br>

                       <table align="center" width="850px">
                          <tbody><tr>
                      <td width="400px">
                                        <center>                                        <p><iframe width="560" height="315" src="./Recycle-GAN_files/sRY46lw4yT4.html" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></p>

                                        </center>
                      </td>
                </tr>
<tr><td width="400px">
                                        <center>
                                <br>                                <span style="font-size:14px"><i>The above video shows our attempt to simulate environmental conditions (clouds and winds on a windy day, or still clouds). Conditioning it for a required setting, we can synthesize the condition without changing the aesthetics of the place.</i><br><br><br><br>
                                        </span></center>
                      </td>

                  </tr></tbody></table>


                       <table align="center" width="850px">
                          <tbody><tr>
                      <td width="400px">
                                        <center>                                        <p><iframe width="560" height="315" src="./Recycle-GAN_files/I4dcHRQVOYs.html" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></p>

                                        </center>
                      </td>
                </tr>
<tr><td width="400px">
                                        <center>
                                <br>                                <span style="font-size:14px"><i>The above video shows our attempt to simulate environmental conditions (clouds and winds on a light breeze, or still clouds). Conditioning it for a required setting, we can synthesize the condition without changing the aesthetics of the place.</i><br><br><br>
                                        </span></center>
                      </td>

                  </tr></tbody></table>


                            </td>
                          </tr>
                        </tbody></table>

                  <hr>

                  <center><h2>Sunrise &amp; Sunset</h2></center><table align="center" width="850px">
                          
                          <tbody><tr>   
                             <td>
			   As humans, we have a tendency to align abstract concepts, and to think/imagine of how something would look like if we were at some other location than the place where we are currently making an observation. E.g., a person might be seeing a sunset in New York on the shores of Atlantic Ocean, and may start imaging how a sunset would look like in California around Pacific; or a person might be roaming in the streets of Pittsburgh and may start to imagine what it would be like roaming in the streets of Paris.<br><br>


			   Inspired by this thought process, we extracted the sunrise and sunset data from various web videos, and show how our approach could be used for both video manipulation and content alignment. This is similar to settings in our experiments on clouds and wind synthesis. <br><br>

                       <table align="center" width="850px">
                          <tbody><tr>
                      <td width="400px">
                                        <center>                                        <p><iframe width="560" height="315" src="./Recycle-GAN_files/_Ql9Dccs6no.html" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></p>

                                        </center>
                      </td>
                </tr>
<tr><td width="400px">
                                        <center>
                                <br>                                <span style="font-size:14px"><i>The above video shows our attempt to synthesize a sunrise at a given place.</i><br><br><br>
                                        </span></center>
                      </td>

                  </tr></tbody></table>


                       <table align="center" width="850px">
                          <tbody><tr>
                      <td width="400px">
                                        <center>                                        <p><iframe width="560" height="315" src="./Recycle-GAN_files/2HI54jujJ8A.html" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></p>

                                        </center>
                      </td>
                </tr>
<tr><td width="400px">
                                        <center>                                <br>                                <span style="font-size:14px"><i>The above video shows our attempt to align the content of sunset at two different locations.</i><br><br><br>
                                        </span></center>
                      </td>

                  </tr></tbody></table>




                            </td>
                          </tr>
                        </tbody></table>

                  <br><br>
                  <hr>
		  
                  <center><h1>Origami-bird learning from Real-bird</h1></center><table align="center" width="850px">
                          
                          <tbody><tr>
                             <td>


                        <table align="center" width="850px">
                          <tbody><tr>
                      <td width="400px">
                                        <center>
                                        <p><iframe width="560" height="315" src="./Recycle-GAN_files/XZc3tf14zDU.html" frameborder="0" allowfullscreen=""></iframe></p>

                                        </center>
                      </td>
                </tr>

                  </tbody></table>

                                        <center>
                                <br>                                <span style="font-size:14px"><i> We took the raw origami bird video from <a href="http://www.cs.cmu.edu/~om3d/">Kholgade et al</a>.</i><br><br><br>
                                        </span></center>

                            </td>
                          </tr>
                        </tbody></table>

                  <br><br>
		  <hr>
	
                  <center><h1>human and robot</h1></center><table align="center" width="850px">
                          
                          <tbody><tr>
                             <td>



                        <table align="center" width="850px">
                          <tbody><tr>
                      <td width="400px">
                                        <center>
                                        <p><iframe width="560" height="315" src="./Recycle-GAN_files/G09GNDWgq0I.html" frameborder="0" allowfullscreen=""></iframe></p>

                                        </center>
                      </td>
                </tr>

                  </tbody></table>

                                        <center>
                                <br>                                <span style="font-size:14px"><i> Made an attempt at learning association between a human and a robot pickup (dataset from <a href="https://arxiv.org/pdf/1810.07121.pdf">Sharma et al</a>).</i><br><br><br>
                                        </span></center>

                            </td>
                          </tr>
                        </tbody></table>

                  <br>
                  <hr>	

                  <center><h1>a summary video created by CMU media folks</h1></center><table align="center" width="850px">
                          
                          <tbody><tr>
                             <td>



                        <table align="center" width="850px">
                          <tbody><tr>
                      <td width="400px">
                                        <center>
                                        <p><iframe width="560" height="315" src="./Recycle-GAN_files/ehD3C60i6lw.html" frameborder="0" allowfullscreen=""></iframe></p>

                                        </center>
                      </td>
                </tr>

                  </tbody></table>

                                        <center>
                                <br>                                <span style="font-size:14px"><i><a href="https://www.youtube.com/watch?v=ehD3C60i6lw">Transferring One Video Into the Style of Another</a>).</i><br><br><br>
                                        </span></center>

                            </td>
                          </tr>
                        </tbody></table>

                  <br>
                  <hr>

  		  <table align="center" width="1100px">
  			  <tbody><tr>
  	              <td width="400px">
  					<left>
	  		  <center><h1>Acknowledgements</h1></center>
			  <center> We thank the authors of <a href="https://junyanz.github.io/CycleGAN/">Cycle-GAN and Pix2Pix</a>, and <a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose">OpenPose</a> for their work. It is because of them, this work could be possible. Inspired from Cycle-GAN, we name our approach Recycle-GAN. We thank the larger community that collected and uploaded the videos on web. It is because of their efforts, we could do this academic research work. Many thanks to our friends and colleagues at CMU and Facebook for many discussions and suggestions. This work could only cover a tiny part of those wonderful suggestions and ideas. Thanks to Bryan Russell, Chia-Yin Tsai, Aravindh Mahendran, Tomas Pfister, Zhe Cao, Martin Li, and Siva Chaitanya for various suggestions on text and videos. Finally, we thank the authors of <a href="http://richzhang.github.io/colorization/">Colorful Image Colorization</a> for this webpage design.</center>

			</left>
		</td>
			 </tr>
		</tbody></table>

		<br><br>

<script>

var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
        document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
        </script><script src="./Recycle-GAN_files/ga.js" type="text/javascript"></script><script src="./Recycle-GAN_files/ga(1).js" type="text/javascript"></script> <script type="text/javascript">
try {
        var pageTracker = _gat._getTracker("UA-39749944-1");
        pageTracker._trackPageview();
        } catch(err) {}

</script>
              


 

</td></tr></tbody></table></td></tr></tbody></table></body></html>