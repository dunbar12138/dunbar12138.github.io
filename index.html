<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Kangle Deng</title>

  <meta name="author" content="Kangle Deng">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/cmu-seal-r.png">
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Kangle Deng</name>
                  </p>
                  <!-- <p>I am a CS undergraudate at <a href="http://english.pku.edu.cn/">Peking University</a> starting from Sep. 2016. For now I'm applying for graduate school in Computer Science.
              </p> -->
                  <p>I am a PhD student at the <a href="http://www.ri.cmu.edu/">Robotics Institute</a> of <a
                      href="http://www.cmu.edu">Carnegie Mellon University</a>, where I'm fortunate to be co-advised by
                    <a href="https://www.cs.cmu.edu/~deva/">Deva Ramanan</a> and <a
                      href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu. </a>
                  </p>
                  <!-- <p>
                I am a research intern at IV-OCR Group, <a href="https://www.sensetime.com/en/">Sensetime AI</a>, starting from Oct. 2019. At Sensetime, I've worked on Form Structuralization and Image Steganography.
              </p> -->
                  <!-- <p>
                I spent last summer as an intern at the <a href="https://www.ri.cmu.edu/">Robotics Institute</a> of <a href="https://www.cmu.edu/">Carnegie Mellon University</a>, where I'm fortunate to be advised by <a href="http://www.cs.cmu.edu/~aayushb/">Aayush Bansal</a> and <a href="https://www.cs.cmu.edu/~deva/">Deva Ramanan</a>.
              </p> -->
                  <p>Prior to joining CMU, I obtained my B.S degree in 2020 from <a
                      href="http://english.pku.edu.cn/">Peking University</a>.</p>
                  <p>
                    So far, my research mainly involves computer-aided creation. My current work is supported by
                    <a href="https://www.microsoft.com/en-us/research/academic-program/phd-fellowship/">Microsoft Research PhD Fellowship</a>.
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:kangled@andrew.cmu.edu">Email</a> &nbsp/&nbsp
                    <!--                 <a href="data/Kangle_CV.pdf">CV</a> &nbsp/&nbsp -->
                    <a href="https://github.com/dunbar12138"> GitHub </a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?hl=en&user=esqd1TgAAAAJ"> Google Scholar </a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a><img style="width:200px;max-width:100%" alt="profile photo" src="images/kangle2020_circle.png"
                      class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <!-- <heading>Research</heading> -->
                  <heading>Selected Publications</heading>
                  <p>
                    <!--                 I'm interested in Multimodal Perception, including Computer Vison and Audio Processing. I'm also interested in applications of Information Theory to Maching Learning.  -->

                    <!--                 So far, my research mainly involves computer-aided creation. My research proposal of <em>'Reconstructing and Synthesizing 3D-Aware Content'</em> won 2022 Microsoft Research PhD Fellowship.  -->
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <video width="310" muted autoplay loop>
                    <source src="images/TactileDreamFusion/avocado_hstack.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <!-- <a href="https://www.ijcai.org/proceedings/2019/0307.pdf">
                <papertitle>IRC-GAN: Introspective Recurrent Convolutional GAN for Text-to-video Generation</papertitle>
              </a> -->
                  <papertitle><a href="https://arxiv.org/abs/2412.06785">Tactile DreamFusion: Exploiting Tactile Sensing for 3D Generation
                  </a></papertitle>
                  <br>
                  <a href="https://ruihangao.github.io/">Ruihan Gao</a>,
                  <strong>Kangle Deng</strong>,
                  <a href="https://gengshan-y.github.io/">Gengshan Yang</a>,
                  <a href="https://siebelschool.illinois.edu/about/people/all-faculty/yuanwz">Wenzhen Yuan</a>,
                  <a href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a>
                  <br> <br>
                  <em>NeurIPS</em>, 2024
                  <!-- <em>ArXiv</em> -->
                  <br>
                  <a href="https://ruihangao.github.io/TactileDreamFusion/">project page</a> / <a
                  href="https://github.com/RuihanGao/TactileDreamFusion">github</a>
                  <p></p>
                  <p>3D content creation with touch: <i>TactileDreamFusion</i> exploits high-resolution tactile sensing to enhance geometric details for text- or image-to-3D generation.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <video width="310" muted autoplay loop>
                    <source src="images/flashtex/studio_bay_fireplace_stacked.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <!-- <a href="https://www.ijcai.org/proceedings/2019/0307.pdf">
                <papertitle>IRC-GAN: Introspective Recurrent Convolutional GAN for Text-to-video Generation</papertitle>
              </a> -->
                  <papertitle><a href="https://arxiv.org/abs/2402.13251">FlashTex: Fast Relightable Mesh Texturing with LightControlNet</a></papertitle>
                  <br>
                  <strong>Kangle Deng</strong>, Timothy Omernick, Alexander Weiss, 
                  <a href="https://www.cs.cmu.edu/~deva/">Deva Ramanan</a>,  
                  <a href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a>, Tinghui Zhou, 
                  <a href="https://graphics.stanford.edu/~maneesh/">Maneesh Agrawala</a>
                  <br> <br>
                  <em>ECCV</em>, 2024 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                  <!-- <em>ArXiv</em> -->
                  <br>
                  <a href="https://flashtex.github.io/">project page</a> / <a
                  href="https://github.com/Roblox/FlashTex">github</a>
                  <p></p>
                  <p>FlashTex textures an input 3D mesh given a user-provided text prompt. Notably, our generated texture can be relit properly in different lighting environments.</p>
                </td>
              </tr>

              <tr onmouseout="totalrecon_stop()" onmouseover="totalrecon_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <video width="310" muted autoplay loop>
                    <source src="images/total_recon/cover_figure.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <!-- <a href="https://www.ijcai.org/proceedings/2019/0307.pdf">
                <papertitle>IRC-GAN: Introspective Recurrent Convolutional GAN for Text-to-video Generation</papertitle>
              </a> -->
                  <papertitle><a href="https://arxiv.org/abs/2304.12317">Total-Recon: Deformable Scene Reconstruction
                      for Embodied View Synthesis</a>
                  </papertitle>
                  <br>
                  <a href="https://andrewsonga.github.io/">Chonghyuk(Andrew) Song</a>, <a
                    href="https://gengshan-y.github.io/">Gengshan Yang</a>, <strong>Kangle Deng</strong>, <a
                    href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a>, <a
                    href="https://www.cs.cmu.edu/~deva/">Deva
                    Ramanan</a>
                  <br> <br>
                  <em>ICCV</em>, 2023
                  <br>
                  <a href="https://andrewsonga.github.io/totalrecon/">project page</a> / <a
                    href="https://github.com/andrewsonga/Total-Recon">github</a>
                  <p></p>
                  <p>Given a RGBD video of deformable objects, <em>Total-Recon</em>
                    builds 3D models of objects and background, which enables embodied view synthesis. </p>
                </td>
              </tr>


              <tr onmouseout="pix2pix3D_stop()" onmouseover="pix2pix3D_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='pix2pix3D_image'><video width="310" muted autoplay loop>
                        <source src="images/pix2pix3d/demo_2.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/pix2pix3d/teaser_jpg.jpg' width="300">
                  </div>
                  <script type="text/javascript">
                    function pix2pix3D_start() {
                      document.getElementById('pix2pix3D_image').style.opacity = "1";
                    }

                    function pix2pix3D_stop() {
                      document.getElementById('pix2pix3D_image').style.opacity = "0";
                    }
                    pix2pix3D_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <!-- <a href="https://www.ijcai.org/proceedings/2019/0307.pdf">
                <papertitle>IRC-GAN: Introspective Recurrent Convolutional GAN for Text-to-video Generation</papertitle>
              </a> -->
                  <papertitle><a href="https://arxiv.org/abs/2302.08509">3D-aware Conditional Image Synthesis</a>
                  </papertitle>
                  <br>
                  <strong>Kangle Deng</strong>, <a href="https://gengshan-y.github.io/">Gengshan Yang</a>, <a
                    href="https://www.cs.cmu.edu/~deva/">Deva Ramanan</a>, <a
                    href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a>
                  <br> <br>
                  <em>CVPR</em>, 2023
                  <br>
                  <a href="http://www.cs.cmu.edu/~pix2pix3D/">project page</a> / <a
                    href="https://github.com/dunbar12138/pix2pix3D">github</a>
                  <p></p>
                  <p>We propose <em>pix2pix3D</em>, a 3D-aware conditional generative model for controllable
                    photorealistic image synthesis. Given a 2D label map, such as a segmentation or edge map, our model
                    learns to synthesize a corresponding image from different viewpoints.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/dsnerf/thumbnail_gif.gif' width="300">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <!-- <a href="https://www.ijcai.org/proceedings/2019/0307.pdf">
                <papertitle>IRC-GAN: Introspective Recurrent Convolutional GAN for Text-to-video Generation</papertitle>
              </a> -->
                  <papertitle><a href="https://arxiv.org/abs/2107.02791">Depth-supervised NeRF: Fewer Views and Faster
                      Training for Free</a></papertitle>
                  <br>
                  <strong>Kangle Deng</strong>, <a href="http://andrewhliu.github.io/">Andrew Liu</a>, <a
                    href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a>, <a
                    href="https://www.cs.cmu.edu/~deva/">Deva Ramanan</a>
                  <br> <br>
                  <em>CVPR</em>, 2022
                  <!-- <em>ArXiv</em> -->
                  <br>
                  <a href="https://www.cs.cmu.edu/~dsnerf/">project page</a> / <a
                    href="https://github.com/dunbar12138/DSNeRF">github</a>
                  <p></p>
                  <p>Proposed DS-NeRF (Depth-supervised Neural Radiance Fields), a model for learning neural radiance
                    fields that takes advantage of depth supervised by 3D point clouds.</p>
                </td>
              </tr>


              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/AudiovisualSynthesis/fig_teaser.jpg' width="300">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <!-- <a href="https://www.ijcai.org/proceedings/2019/0307.pdf">
                <papertitle>IRC-GAN: Introspective Recurrent Convolutional GAN for Text-to-video Generation</papertitle>
              </a> -->
                  <papertitle><a href="https://arxiv.org/abs/2001.04463">Unsupervised Any-to-Many Audiovisual Synthesis
                      via Exemplar Autoencoders</a></papertitle>
                  <br>
                  <strong>Kangle Deng</strong>, <a href="http://www.cs.cmu.edu/~aayushb/">Aayush Bansal</a>, <a
                    href="https://www.cs.cmu.edu/~deva/">Deva Ramanan</a>
                  <br> <br>
                  <em>ICLR</em>, 2021
                  <!-- <em>ArXiv</em> -->
                  <br>
                  <a href="https://www.cs.cmu.edu/~exemplar-ae/">project page</a>
                  <p></p>
                  <p>Defined and addressed a new question of unsupervised audiovisual synthesis -- input the audio of a
                    random individual and then output the talking-head video with audio in the style of another target
                    speaker.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/Text2Video/Teaser.jpeg' width="300">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://www.ijcai.org/proceedings/2019/0307.pdf">
                    <papertitle>IRC-GAN: Introspective Recurrent Convolutional GAN for Text-to-video Generation
                    </papertitle>
                  </a>
                  <br>
                  <strong>Kangle Deng</strong>*, Tianyi Fei*, Xin Huang, <a
                    href="http://59.108.48.34/tiki/yuxinpeng/">Yuxin Peng</a>
                  <br> <br>
                  <em>IJCAI</em>, 2019
                  <br>
                  <a href="bibtex/ircgan.bib">bibtex</a>
                  <p></p>
                  <p>Application of Mutual Information to Text-to-video Generation.</p>
                </td>
              </tr>

            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Selected Honors and Awards</heading>
                  <ul>
                    <li>Microsoft Research Fellowship Award, 2022; </li>
                  </ul>
                  <ul>
                    <li>Outstanding Graduates in Beijing, 2020;</li>
                  </ul>
                  <ul>
                    <li>Peking University Weiming Scholar, 2020;</li>
                  </ul>
                  <ul>
                    <li>China National Scholarship (Top 1%, 3 times) , 2017, 2018, and 2019;</li>
                  </ul>
                  <ul>
                    <li>Sensetime Scholarship, 2019;</li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Teaching</heading>
                  <p>
                    Teaching Assistant:
                  </p>
                  <ul>
                    <li>Spring 2023, 16-720A: Computer Vision, Carnegie Mellon University</li>
                  </ul>
                  <ul>
                    <li>Fall 2022, 16-822: Geometry-based Methods in Vision, Carnegie Mellon University</li>
                  </ul>
                  <ul>
                    <li>Fall 2018, Introduction to Computer Systems, Peking University</li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:center;font-size:small;">
                    This webpage is "stolen" from <a href="https://github.com/jonbarron/jonbarron_website">Jon
                      Barron</a>. Thanks to him!
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>